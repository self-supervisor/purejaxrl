{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# import jax.numpy as jnp\n",
    "# env = gym.make('CartPole-v1')\n",
    "\n",
    "# states = []\n",
    "# actions = []\n",
    "# next_states = []\n",
    "\n",
    "# for _ in range(1000):\n",
    "#     obs, _ = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         actions.append(action)\n",
    "#         new_obs, _, terminated, truncated, _ = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         next_states.append(new_obs)\n",
    "#         states.append(obs)\n",
    "#         obs = new_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/augustine/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/home/augustine/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/home/augustine/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 2.1585497856140137\n",
      "Epoch 2 - Training Loss: 1.830747127532959\n",
      "Epoch 3 - Training Loss: 1.550766944885254\n",
      "Epoch 4 - Training Loss: 1.3095893859863281\n",
      "Epoch 5 - Training Loss: 1.1013367176055908\n",
      "Epoch 6 - Training Loss: 0.9216362237930298\n",
      "Epoch 7 - Training Loss: 0.7670014500617981\n",
      "Epoch 8 - Training Loss: 0.6344825029373169\n",
      "Epoch 9 - Training Loss: 0.5215258598327637\n",
      "Epoch 10 - Training Loss: 0.42594948410987854\n",
      "Epoch 11 - Training Loss: 0.34591734409332275\n",
      "Epoch 12 - Training Loss: 0.27981871366500854\n",
      "Epoch 13 - Training Loss: 0.22610893845558167\n",
      "Epoch 14 - Training Loss: 0.18318219482898712\n",
      "Epoch 15 - Training Loss: 0.14934857189655304\n",
      "Epoch 16 - Training Loss: 0.1229403167963028\n",
      "Epoch 17 - Training Loss: 0.10246969014406204\n",
      "Epoch 18 - Training Loss: 0.08672597259283066\n",
      "Epoch 19 - Training Loss: 0.07476284354925156\n",
      "Epoch 20 - Training Loss: 0.06582226604223251\n",
      "Epoch 21 - Training Loss: 0.05924617126584053\n",
      "Epoch 22 - Training Loss: 0.05442427843809128\n",
      "Epoch 23 - Training Loss: 0.05079585313796997\n",
      "Epoch 24 - Training Loss: 0.04789305850863457\n",
      "Epoch 25 - Training Loss: 0.045385804027318954\n",
      "Epoch 26 - Training Loss: 0.04308556765317917\n",
      "Epoch 27 - Training Loss: 0.040906865149736404\n",
      "Epoch 28 - Training Loss: 0.03881831094622612\n",
      "Epoch 29 - Training Loss: 0.03680955991148949\n",
      "Epoch 30 - Training Loss: 0.03487693890929222\n",
      "Epoch 31 - Training Loss: 0.03301846981048584\n",
      "Epoch 32 - Training Loss: 0.03123239427804947\n",
      "Epoch 33 - Training Loss: 0.02951708808541298\n",
      "Epoch 34 - Training Loss: 0.027870913967490196\n",
      "Epoch 35 - Training Loss: 0.026292268186807632\n",
      "Epoch 36 - Training Loss: 0.024779532104730606\n",
      "Epoch 37 - Training Loss: 0.02333127148449421\n",
      "Epoch 38 - Training Loss: 0.021945973858237267\n",
      "Epoch 39 - Training Loss: 0.02062225341796875\n",
      "Epoch 40 - Training Loss: 0.01935870386660099\n",
      "Epoch 41 - Training Loss: 0.018154071643948555\n",
      "Epoch 42 - Training Loss: 0.017007017508149147\n",
      "Epoch 43 - Training Loss: 0.01591632142663002\n",
      "Epoch 44 - Training Loss: 0.014880713075399399\n",
      "Epoch 45 - Training Loss: 0.01389896310865879\n",
      "Epoch 46 - Training Loss: 0.012969838455319405\n",
      "Epoch 47 - Training Loss: 0.01209210604429245\n",
      "Epoch 48 - Training Loss: 0.011264491826295853\n",
      "Epoch 49 - Training Loss: 0.01048573199659586\n",
      "Epoch 50 - Training Loss: 0.009754507802426815\n",
      "Epoch 51 - Training Loss: 0.00906953401863575\n",
      "Epoch 52 - Training Loss: 0.008429443463683128\n",
      "Epoch 53 - Training Loss: 0.007832877337932587\n",
      "Epoch 54 - Training Loss: 0.0072784340009093285\n",
      "Epoch 55 - Training Loss: 0.006764595862478018\n",
      "Epoch 56 - Training Loss: 0.0062895589508116245\n",
      "Epoch 57 - Training Loss: 0.005851006135344505\n",
      "Epoch 58 - Training Loss: 0.005446361377835274\n",
      "Epoch 59 - Training Loss: 0.005073165986686945\n",
      "Epoch 60 - Training Loss: 0.004729222506284714\n",
      "Epoch 61 - Training Loss: 0.004412509500980377\n",
      "Epoch 62 - Training Loss: 0.004121159669011831\n",
      "Epoch 63 - Training Loss: 0.0038534326013177633\n",
      "Epoch 64 - Training Loss: 0.003607681952416897\n",
      "Epoch 65 - Training Loss: 0.0033823964186012745\n",
      "Epoch 66 - Training Loss: 0.003176203928887844\n",
      "Epoch 67 - Training Loss: 0.002987887477502227\n",
      "Epoch 68 - Training Loss: 0.002816382795572281\n",
      "Epoch 69 - Training Loss: 0.0026607841718941927\n",
      "Epoch 70 - Training Loss: 0.002520260401070118\n",
      "Epoch 71 - Training Loss: 0.002393891802057624\n",
      "Epoch 72 - Training Loss: 0.0022804811596870422\n",
      "Epoch 73 - Training Loss: 0.0021787069272249937\n",
      "Epoch 74 - Training Loss: 0.0020874529145658016\n",
      "Epoch 75 - Training Loss: 0.002005877671763301\n",
      "Epoch 76 - Training Loss: 0.001933151506818831\n",
      "Epoch 77 - Training Loss: 0.0018681231886148453\n",
      "Epoch 78 - Training Loss: 0.0018093585968017578\n",
      "Epoch 79 - Training Loss: 0.001755529665388167\n",
      "Epoch 80 - Training Loss: 0.001705616363324225\n",
      "Epoch 81 - Training Loss: 0.0016588864382356405\n",
      "Epoch 82 - Training Loss: 0.001614812994375825\n",
      "Epoch 83 - Training Loss: 0.0015729863662272692\n",
      "Epoch 84 - Training Loss: 0.0015330907190218568\n",
      "Epoch 85 - Training Loss: 0.001494867610745132\n",
      "Epoch 86 - Training Loss: 0.00145811028778553\n",
      "Epoch 87 - Training Loss: 0.001422649947926402\n",
      "Epoch 88 - Training Loss: 0.0013883423525840044\n",
      "Epoch 89 - Training Loss: 0.0013550720177590847\n",
      "Epoch 90 - Training Loss: 0.0013227384770289063\n",
      "Epoch 91 - Training Loss: 0.0012912548845633864\n",
      "Epoch 92 - Training Loss: 0.0012605497613549232\n",
      "Epoch 93 - Training Loss: 0.0012305629206821322\n",
      "Epoch 94 - Training Loss: 0.0012012419756501913\n",
      "Epoch 95 - Training Loss: 0.00117254548240453\n",
      "Epoch 96 - Training Loss: 0.0011444400297477841\n",
      "Epoch 97 - Training Loss: 0.0011168966302648187\n",
      "Epoch 98 - Training Loss: 0.0010898895561695099\n",
      "Epoch 99 - Training Loss: 0.0010633913334459066\n",
      "Epoch 100 - Training Loss: 0.001037383684888482\n",
      "Epoch 101 - Training Loss: 0.001011838554404676\n",
      "Epoch 102 - Training Loss: 0.0009867491899058223\n",
      "Epoch 103 - Training Loss: 0.0009620948112569749\n",
      "Epoch 104 - Training Loss: 0.0009378682007081807\n",
      "Epoch 105 - Training Loss: 0.0009140711626969278\n",
      "Epoch 106 - Training Loss: 0.000890674942638725\n",
      "Epoch 107 - Training Loss: 0.0008676962461322546\n",
      "Epoch 108 - Training Loss: 0.0008451155736111104\n",
      "Epoch 109 - Training Loss: 0.0008229368249885738\n",
      "Epoch 110 - Training Loss: 0.0008011659374460578\n",
      "Epoch 111 - Training Loss: 0.0007797569269314408\n",
      "Epoch 112 - Training Loss: 0.0007587503059767187\n",
      "Epoch 113 - Training Loss: 0.0007381288451142609\n",
      "Epoch 114 - Training Loss: 0.0007178955129347742\n",
      "Epoch 115 - Training Loss: 0.0006980311591178179\n",
      "Epoch 116 - Training Loss: 0.0006785495788790286\n",
      "Epoch 117 - Training Loss: 0.0006594420410692692\n",
      "Epoch 118 - Training Loss: 0.0006407171604223549\n",
      "Epoch 119 - Training Loss: 0.0006223574164323509\n",
      "Epoch 120 - Training Loss: 0.0006043576286174357\n",
      "Epoch 121 - Training Loss: 0.0005867342697456479\n",
      "Epoch 122 - Training Loss: 0.0005694783758372068\n",
      "Epoch 123 - Training Loss: 0.0005525852902792394\n",
      "Epoch 124 - Training Loss: 0.0005360596696846187\n",
      "Epoch 125 - Training Loss: 0.0005198823637329042\n",
      "Epoch 126 - Training Loss: 0.0005040629184804857\n",
      "Epoch 127 - Training Loss: 0.0004885999951511621\n",
      "Epoch 128 - Training Loss: 0.0004735004040412605\n",
      "Epoch 129 - Training Loss: 0.00045871775364503264\n",
      "Epoch 130 - Training Loss: 0.0004442889185156673\n",
      "Epoch 131 - Training Loss: 0.00043019288568757474\n",
      "Epoch 132 - Training Loss: 0.0004164371348451823\n",
      "Epoch 133 - Training Loss: 0.00040296430233865976\n",
      "Epoch 134 - Training Loss: 0.0003898155700881034\n",
      "Epoch 135 - Training Loss: 0.00037696651997976005\n",
      "Epoch 136 - Training Loss: 0.00036441590054892004\n",
      "Epoch 137 - Training Loss: 0.0003521559410728514\n",
      "Epoch 138 - Training Loss: 0.0003401772410143167\n",
      "Epoch 139 - Training Loss: 0.00032849537092261016\n",
      "Epoch 140 - Training Loss: 0.00031708076130598783\n",
      "Epoch 141 - Training Loss: 0.0003059493319597095\n",
      "Epoch 142 - Training Loss: 0.0002950791676994413\n",
      "Epoch 143 - Training Loss: 0.00028449337696656585\n",
      "Epoch 144 - Training Loss: 0.00027413436328060925\n",
      "Epoch 145 - Training Loss: 0.0002640441234689206\n",
      "Epoch 146 - Training Loss: 0.000254203740041703\n",
      "Epoch 147 - Training Loss: 0.00024462028522975743\n",
      "Epoch 148 - Training Loss: 0.00023526103177573532\n",
      "Epoch 149 - Training Loss: 0.000226157862925902\n",
      "Epoch 150 - Training Loss: 0.00021729880245402455\n",
      "Epoch 151 - Training Loss: 0.00020867878629360348\n",
      "Epoch 152 - Training Loss: 0.0002002956171054393\n",
      "Epoch 153 - Training Loss: 0.00019216732471249998\n",
      "Epoch 154 - Training Loss: 0.0001842543133534491\n",
      "Epoch 155 - Training Loss: 0.00017659197328612208\n",
      "Epoch 156 - Training Loss: 0.0001691643992671743\n",
      "Epoch 157 - Training Loss: 0.00016198265075217932\n",
      "Epoch 158 - Training Loss: 0.00015501215239055455\n",
      "Epoch 159 - Training Loss: 0.00014829305291641504\n",
      "Epoch 160 - Training Loss: 0.000141786367748864\n",
      "Epoch 161 - Training Loss: 0.0001355116837657988\n",
      "Epoch 162 - Training Loss: 0.0001294696849072352\n",
      "Epoch 163 - Training Loss: 0.00012363384303171188\n",
      "Epoch 164 - Training Loss: 0.00011801499931607395\n",
      "Epoch 165 - Training Loss: 0.00011261555482633412\n",
      "Epoch 166 - Training Loss: 0.00010740831203293055\n",
      "Epoch 167 - Training Loss: 0.00010239682887913659\n",
      "Epoch 168 - Training Loss: 9.757930092746392e-05\n",
      "Epoch 169 - Training Loss: 9.293872426496819e-05\n",
      "Epoch 170 - Training Loss: 8.845388219924644e-05\n",
      "Epoch 171 - Training Loss: 8.413790055783466e-05\n",
      "Epoch 172 - Training Loss: 7.996480417205021e-05\n",
      "Epoch 173 - Training Loss: 7.595702481921762e-05\n",
      "Epoch 174 - Training Loss: 7.208704482764006e-05\n",
      "Epoch 175 - Training Loss: 6.837426190031692e-05\n",
      "Epoch 176 - Training Loss: 6.48016357445158e-05\n",
      "Epoch 177 - Training Loss: 6.138269964139909e-05\n",
      "Epoch 178 - Training Loss: 5.8090867241844535e-05\n",
      "Epoch 179 - Training Loss: 5.4946263844612986e-05\n",
      "Epoch 180 - Training Loss: 5.1940613047918305e-05\n",
      "Epoch 181 - Training Loss: 4.9050257075577974e-05\n",
      "Epoch 182 - Training Loss: 4.6290682803373784e-05\n",
      "Epoch 183 - Training Loss: 4.3667805584846064e-05\n",
      "Epoch 184 - Training Loss: 4.1125400457531214e-05\n",
      "Epoch 185 - Training Loss: 3.8708407373633236e-05\n",
      "Epoch 186 - Training Loss: 3.6386587453307584e-05\n",
      "Epoch 187 - Training Loss: 3.416731851757504e-05\n",
      "Epoch 188 - Training Loss: 3.203057713108137e-05\n",
      "Epoch 189 - Training Loss: 2.998245690832846e-05\n",
      "Epoch 190 - Training Loss: 2.8023228878737427e-05\n",
      "Epoch 191 - Training Loss: 2.613622746139299e-05\n",
      "Epoch 192 - Training Loss: 2.4347100406885147e-05\n",
      "Epoch 193 - Training Loss: 2.2619538867729716e-05\n",
      "Epoch 194 - Training Loss: 2.0975281586288475e-05\n",
      "Epoch 195 - Training Loss: 1.9418135707383044e-05\n",
      "Epoch 196 - Training Loss: 1.793830051610712e-05\n",
      "Epoch 197 - Training Loss: 1.653508297749795e-05\n",
      "Epoch 198 - Training Loss: 1.5206113857857417e-05\n",
      "Epoch 199 - Training Loss: 1.3957092960481532e-05\n",
      "Epoch 200 - Training Loss: 1.2784060345438775e-05\n",
      "Epoch 201 - Training Loss: 1.1695839020831045e-05\n",
      "Epoch 202 - Training Loss: 1.0674745681171771e-05\n",
      "Epoch 203 - Training Loss: 9.72477482719114e-06\n",
      "Epoch 204 - Training Loss: 8.860747584549244e-06\n",
      "Epoch 205 - Training Loss: 8.060212167038117e-06\n",
      "Epoch 206 - Training Loss: 7.340217507589841e-06\n",
      "Epoch 207 - Training Loss: 6.680713340756483e-06\n",
      "Epoch 208 - Training Loss: 6.101824965298874e-06\n",
      "Epoch 209 - Training Loss: 5.5916184464877006e-06\n",
      "Epoch 210 - Training Loss: 5.137077096151188e-06\n",
      "Epoch 211 - Training Loss: 4.755806912726257e-06\n",
      "Epoch 212 - Training Loss: 4.438781161297811e-06\n",
      "Epoch 213 - Training Loss: 4.170821284787962e-06\n",
      "Epoch 214 - Training Loss: 3.966154054069193e-06\n",
      "Epoch 215 - Training Loss: 3.8122777823446086e-06\n",
      "Epoch 216 - Training Loss: 3.701093646668596e-06\n",
      "Epoch 217 - Training Loss: 3.628176045822329e-06\n",
      "Epoch 218 - Training Loss: 3.583031457310426e-06\n",
      "Epoch 219 - Training Loss: 3.5679324810189428e-06\n",
      "Epoch 220 - Training Loss: 3.565341557987267e-06\n",
      "Epoch 221 - Training Loss: 3.556432375262375e-06\n",
      "Epoch 222 - Training Loss: 3.5597583973867586e-06\n",
      "Epoch 223 - Training Loss: 3.5607140489446465e-06\n",
      "Epoch 224 - Training Loss: 3.559388233043137e-06\n",
      "Epoch 225 - Training Loss: 3.557294803613331e-06\n",
      "Epoch 226 - Training Loss: 3.5639386624097824e-06\n",
      "Epoch 227 - Training Loss: 3.5580023904913105e-06\n",
      "Epoch 228 - Training Loss: 3.5602529351308476e-06\n",
      "Epoch 229 - Training Loss: 3.556647925506695e-06\n",
      "Epoch 230 - Training Loss: 3.5688522075361107e-06\n",
      "Epoch 231 - Training Loss: 3.557912350515835e-06\n",
      "Epoch 232 - Training Loss: 3.559963261068333e-06\n",
      "Epoch 233 - Training Loss: 3.556459660103428e-06\n",
      "Epoch 234 - Training Loss: 3.5667617339640856e-06\n",
      "Epoch 235 - Training Loss: 3.558895741662127e-06\n",
      "Epoch 236 - Training Loss: 3.5599921375251142e-06\n",
      "Epoch 237 - Training Loss: 3.5573586956161307e-06\n",
      "Epoch 238 - Training Loss: 3.564785401977133e-06\n",
      "Epoch 239 - Training Loss: 3.562871370377252e-06\n",
      "Epoch 240 - Training Loss: 3.5577716062107356e-06\n",
      "Epoch 241 - Training Loss: 3.556710453267442e-06\n",
      "Epoch 242 - Training Loss: 3.5689747619471746e-06\n",
      "Epoch 243 - Training Loss: 3.5632178878586274e-06\n",
      "Epoch 244 - Training Loss: 3.5559432944864966e-06\n",
      "Epoch 245 - Training Loss: 3.5650566587719368e-06\n",
      "Epoch 246 - Training Loss: 3.5606910842034267e-06\n",
      "Epoch 247 - Training Loss: 3.559028527888586e-06\n",
      "Epoch 248 - Training Loss: 3.5567882150644436e-06\n",
      "Epoch 249 - Training Loss: 3.5708640098164324e-06\n",
      "Epoch 250 - Training Loss: 3.5639109228213783e-06\n",
      "Epoch 251 - Training Loss: 3.556335968823987e-06\n",
      "Epoch 252 - Training Loss: 3.5623049825517228e-06\n",
      "Epoch 253 - Training Loss: 3.5568809835240245e-06\n",
      "Epoch 254 - Training Loss: 3.5606474284577416e-06\n",
      "Epoch 255 - Training Loss: 3.564739699868369e-06\n",
      "Epoch 256 - Training Loss: 3.5574446428654483e-06\n",
      "Epoch 257 - Training Loss: 3.5676830520969816e-06\n",
      "Epoch 258 - Training Loss: 3.555936928023584e-06\n",
      "Epoch 259 - Training Loss: 3.5649718483909965e-06\n",
      "Epoch 260 - Training Loss: 3.5565956295613432e-06\n",
      "Epoch 261 - Training Loss: 3.563423888408579e-06\n",
      "Epoch 262 - Training Loss: 3.5629568628792185e-06\n",
      "Epoch 263 - Training Loss: 3.5718931030714884e-06\n",
      "Epoch 264 - Training Loss: 3.555936928023584e-06\n",
      "Epoch 265 - Training Loss: 3.5593432130553992e-06\n",
      "Epoch 266 - Training Loss: 3.5596992802311433e-06\n",
      "Epoch 267 - Training Loss: 3.564539156286628e-06\n",
      "Epoch 268 - Training Loss: 3.5599848615675e-06\n",
      "Epoch 269 - Training Loss: 3.5603518426796654e-06\n",
      "Epoch 270 - Training Loss: 3.5582722830440616e-06\n",
      "Epoch 271 - Training Loss: 3.564055987226311e-06\n",
      "Epoch 272 - Training Loss: 3.5607670270110248e-06\n",
      "Epoch 273 - Training Loss: 3.55936163032311e-06\n",
      "Epoch 274 - Training Loss: 3.557039690349484e-06\n",
      "Epoch 275 - Training Loss: 3.565232191249379e-06\n",
      "Epoch 276 - Training Loss: 3.562104438969982e-06\n",
      "Epoch 277 - Training Loss: 3.5576915706769796e-06\n",
      "Epoch 278 - Training Loss: 3.5608613870863337e-06\n",
      "Epoch 279 - Training Loss: 3.5565301459428156e-06\n",
      "Epoch 280 - Training Loss: 3.5713292163563892e-06\n",
      "Epoch 281 - Training Loss: 3.5663533708429895e-06\n",
      "Epoch 282 - Training Loss: 3.5559412481234176e-06\n",
      "Epoch 283 - Training Loss: 3.5566979477152927e-06\n",
      "Epoch 284 - Training Loss: 3.5688397019839613e-06\n",
      "Epoch 285 - Training Loss: 3.5590355764725246e-06\n",
      "Epoch 286 - Training Loss: 3.5566572478273883e-06\n",
      "Epoch 287 - Training Loss: 3.5589625895227073e-06\n",
      "Epoch 288 - Training Loss: 3.5685402508534025e-06\n",
      "Epoch 289 - Training Loss: 3.55596807821712e-06\n",
      "Epoch 290 - Training Loss: 3.5617051707959035e-06\n",
      "Epoch 291 - Training Loss: 3.5625930649985094e-06\n",
      "Epoch 292 - Training Loss: 3.5593075153883547e-06\n",
      "Epoch 293 - Training Loss: 3.5568846215028316e-06\n",
      "Epoch 294 - Training Loss: 3.567435442164424e-06\n",
      "Epoch 295 - Training Loss: 3.5597702208178816e-06\n",
      "Epoch 296 - Training Loss: 3.555997409421252e-06\n",
      "Epoch 297 - Training Loss: 3.563685595509014e-06\n",
      "Epoch 298 - Training Loss: 3.560639925126452e-06\n",
      "Epoch 299 - Training Loss: 3.5564614790928317e-06\n",
      "Epoch 300 - Training Loss: 3.5736049994739005e-06\n",
      "Epoch 301 - Training Loss: 3.5591299365478335e-06\n",
      "Epoch 302 - Training Loss: 3.555936245902558e-06\n",
      "Epoch 303 - Training Loss: 3.5574801131588174e-06\n",
      "Epoch 304 - Training Loss: 3.5629941521619912e-06\n",
      "Epoch 305 - Training Loss: 3.557766149242525e-06\n",
      "Epoch 306 - Training Loss: 3.558019670890644e-06\n",
      "Epoch 307 - Training Loss: 3.5653160921356175e-06\n",
      "Epoch 308 - Training Loss: 3.556507181201596e-06\n",
      "Epoch 309 - Training Loss: 3.5647483400680358e-06\n",
      "Epoch 310 - Training Loss: 3.5659450077218935e-06\n",
      "Epoch 311 - Training Loss: 3.5562393350119237e-06\n",
      "Epoch 312 - Training Loss: 3.562404572221567e-06\n",
      "Epoch 313 - Training Loss: 3.5655962165037636e-06\n",
      "Epoch 314 - Training Loss: 3.5619129903352587e-06\n",
      "Epoch 315 - Training Loss: 3.556116325853509e-06\n",
      "Epoch 316 - Training Loss: 3.565946826711297e-06\n",
      "Epoch 317 - Training Loss: 3.565045972209191e-06\n",
      "Epoch 318 - Training Loss: 3.5582136206357973e-06\n",
      "Epoch 319 - Training Loss: 3.564208782336209e-06\n",
      "Epoch 320 - Training Loss: 3.5580935673351632e-06\n",
      "Epoch 321 - Training Loss: 3.56016948899196e-06\n",
      "Epoch 322 - Training Loss: 3.55986162503541e-06\n",
      "Epoch 323 - Training Loss: 3.559548758858e-06\n",
      "Epoch 324 - Training Loss: 3.5625655527837807e-06\n",
      "Epoch 325 - Training Loss: 3.5621890219772467e-06\n",
      "Epoch 326 - Training Loss: 3.5626239878183696e-06\n",
      "Epoch 327 - Training Loss: 3.5642417515191482e-06\n",
      "Epoch 328 - Training Loss: 3.5653042687044945e-06\n",
      "Epoch 329 - Training Loss: 3.5592190670286072e-06\n",
      "Epoch 330 - Training Loss: 3.558614707799279e-06\n",
      "Epoch 331 - Training Loss: 3.5598209251475055e-06\n",
      "Epoch 332 - Training Loss: 3.5612424653663766e-06\n",
      "Epoch 333 - Training Loss: 3.5647619824885624e-06\n",
      "Epoch 334 - Training Loss: 3.558062644515303e-06\n",
      "Epoch 335 - Training Loss: 3.568224428818212e-06\n",
      "Epoch 336 - Training Loss: 3.556610408850247e-06\n",
      "Epoch 337 - Training Loss: 3.5597331589087844e-06\n",
      "Epoch 338 - Training Loss: 3.5634759569802554e-06\n",
      "Epoch 339 - Training Loss: 3.5694804410013603e-06\n",
      "Epoch 340 - Training Loss: 3.5567727536545135e-06\n",
      "Epoch 341 - Training Loss: 3.5702232707990333e-06\n",
      "Epoch 342 - Training Loss: 3.5645316529553384e-06\n",
      "Epoch 343 - Training Loss: 3.5631949231174076e-06\n",
      "Epoch 344 - Training Loss: 3.555941930244444e-06\n",
      "Epoch 345 - Training Loss: 3.5601301533461083e-06\n",
      "Epoch 346 - Training Loss: 3.5680488963407697e-06\n",
      "Epoch 347 - Training Loss: 3.559786364348838e-06\n",
      "Epoch 348 - Training Loss: 3.5592083804658614e-06\n",
      "Epoch 349 - Training Loss: 3.5608782127383165e-06\n",
      "Epoch 350 - Training Loss: 3.559254309948301e-06\n",
      "Epoch 351 - Training Loss: 3.561684934538789e-06\n",
      "Epoch 352 - Training Loss: 3.556712499630521e-06\n",
      "Epoch 353 - Training Loss: 3.5648015455080895e-06\n",
      "Epoch 354 - Training Loss: 3.5669152111950098e-06\n",
      "Epoch 355 - Training Loss: 3.5564551126299193e-06\n",
      "Epoch 356 - Training Loss: 3.557565605660784e-06\n",
      "Epoch 357 - Training Loss: 3.5588668652053457e-06\n",
      "Epoch 358 - Training Loss: 3.566149871403468e-06\n",
      "Epoch 359 - Training Loss: 3.556850970198866e-06\n",
      "Epoch 360 - Training Loss: 3.559520564522245e-06\n",
      "Epoch 361 - Training Loss: 3.5581379052018747e-06\n",
      "Epoch 362 - Training Loss: 3.563590098565328e-06\n",
      "Epoch 363 - Training Loss: 3.559963715815684e-06\n",
      "Epoch 364 - Training Loss: 3.558743856046931e-06\n",
      "Epoch 365 - Training Loss: 3.560838422345114e-06\n",
      "Epoch 366 - Training Loss: 3.5581665542849805e-06\n",
      "Epoch 367 - Training Loss: 3.558036496542627e-06\n",
      "Epoch 368 - Training Loss: 3.568113697838271e-06\n",
      "Epoch 369 - Training Loss: 3.5653847589856014e-06\n",
      "Epoch 370 - Training Loss: 3.5572934393712785e-06\n",
      "Epoch 371 - Training Loss: 3.556992623998667e-06\n",
      "Epoch 372 - Training Loss: 3.569009777493193e-06\n",
      "Epoch 373 - Training Loss: 3.5583368571678875e-06\n",
      "Epoch 374 - Training Loss: 3.556269348337082e-06\n",
      "Epoch 375 - Training Loss: 3.5605601169663714e-06\n",
      "Epoch 376 - Training Loss: 3.5650298286782345e-06\n",
      "Epoch 377 - Training Loss: 3.562943220458692e-06\n",
      "Epoch 378 - Training Loss: 3.561127869033953e-06\n",
      "Epoch 379 - Training Loss: 3.567398152881651e-06\n",
      "Epoch 380 - Training Loss: 3.555936245902558e-06\n",
      "Epoch 381 - Training Loss: 3.5595514873421052e-06\n",
      "Epoch 382 - Training Loss: 3.5613074942375533e-06\n",
      "Epoch 383 - Training Loss: 3.5581215342972428e-06\n",
      "Epoch 384 - Training Loss: 3.5655464216688415e-06\n",
      "Epoch 385 - Training Loss: 3.5671648674906464e-06\n",
      "Epoch 386 - Training Loss: 3.5575869787862757e-06\n",
      "Epoch 387 - Training Loss: 3.5561683944251854e-06\n",
      "Epoch 388 - Training Loss: 3.561474613889004e-06\n",
      "Epoch 389 - Training Loss: 3.5614780244941358e-06\n",
      "Epoch 390 - Training Loss: 3.559329798008548e-06\n",
      "Epoch 391 - Training Loss: 3.5582527289079735e-06\n",
      "Epoch 392 - Training Loss: 3.5643531646201154e-06\n",
      "Epoch 393 - Training Loss: 3.559019432941568e-06\n",
      "Epoch 394 - Training Loss: 3.560080358511186e-06\n",
      "Epoch 395 - Training Loss: 3.5641094200400403e-06\n",
      "Epoch 396 - Training Loss: 3.56131863554765e-06\n",
      "Epoch 397 - Training Loss: 3.559709966793889e-06\n",
      "Epoch 398 - Training Loss: 3.5575667425291613e-06\n",
      "Epoch 399 - Training Loss: 3.5680616292665945e-06\n",
      "Epoch 400 - Training Loss: 3.5568818930187263e-06\n",
      "Epoch 401 - Training Loss: 3.564793132682098e-06\n",
      "Epoch 402 - Training Loss: 3.562705842341529e-06\n",
      "Epoch 403 - Training Loss: 3.5654913972393842e-06\n",
      "Epoch 404 - Training Loss: 3.5560367450671038e-06\n",
      "Epoch 405 - Training Loss: 3.5621715142042376e-06\n",
      "Epoch 406 - Training Loss: 3.5615389606391545e-06\n",
      "Epoch 407 - Training Loss: 3.55601696355734e-06\n",
      "Epoch 408 - Training Loss: 3.558103117029532e-06\n",
      "Epoch 409 - Training Loss: 3.5713453598873457e-06\n",
      "Epoch 410 - Training Loss: 3.55664269591216e-06\n",
      "Epoch 411 - Training Loss: 3.5627440411190037e-06\n",
      "Epoch 412 - Training Loss: 3.5588227547123097e-06\n",
      "Epoch 413 - Training Loss: 3.5636878692457685e-06\n",
      "Epoch 414 - Training Loss: 3.556362571544014e-06\n",
      "Epoch 415 - Training Loss: 3.5680589007824892e-06\n",
      "Epoch 416 - Training Loss: 3.5674854643730214e-06\n",
      "Epoch 417 - Training Loss: 3.555938292265637e-06\n",
      "Epoch 418 - Training Loss: 3.555967168722418e-06\n",
      "Epoch 419 - Training Loss: 3.5658665638038656e-06\n",
      "Epoch 420 - Training Loss: 3.558007847459521e-06\n",
      "Epoch 421 - Training Loss: 3.561060566426022e-06\n",
      "Epoch 422 - Training Loss: 3.562732445061556e-06\n",
      "Epoch 423 - Training Loss: 3.558454181984416e-06\n",
      "Epoch 424 - Training Loss: 3.5682000998349395e-06\n",
      "Epoch 425 - Training Loss: 3.555946022970602e-06\n",
      "Epoch 426 - Training Loss: 3.5620489597931737e-06\n",
      "Epoch 427 - Training Loss: 3.5636135180538986e-06\n",
      "Epoch 428 - Training Loss: 3.5621205825009383e-06\n",
      "Epoch 429 - Training Loss: 3.5663445032696472e-06\n",
      "Epoch 430 - Training Loss: 3.5559421576181194e-06\n",
      "Epoch 431 - Training Loss: 3.5568327803048305e-06\n",
      "Epoch 432 - Training Loss: 3.5727989597944543e-06\n",
      "Epoch 433 - Training Loss: 3.5572820706875063e-06\n",
      "Epoch 434 - Training Loss: 3.55973088517203e-06\n",
      "Epoch 435 - Training Loss: 3.5559510251914617e-06\n",
      "Epoch 436 - Training Loss: 3.564403641576064e-06\n",
      "Epoch 437 - Training Loss: 3.5592402127804235e-06\n",
      "Epoch 438 - Training Loss: 3.5561074582801666e-06\n",
      "Epoch 439 - Training Loss: 3.568056172298384e-06\n",
      "Epoch 440 - Training Loss: 3.5582245345722185e-06\n",
      "Epoch 441 - Training Loss: 3.5569055398809724e-06\n",
      "Epoch 442 - Training Loss: 3.5647103686642367e-06\n",
      "Epoch 443 - Training Loss: 3.5583552744355984e-06\n",
      "Epoch 444 - Training Loss: 3.5580808344093384e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_307943/3909750534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# / jnp.array([10, 1, 1, 1, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# / jnp.array([10, 1, 1, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlax_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewriting_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mlax_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewriting_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3890\u001b[0m         \u001b[0;31m# Use dynamic rather than static slice here to avoid slow repeated execution:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;31m# See https://github.com/google/jax/issues/12198\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3892\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_slice_in_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3893\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3894\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_in_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/lax/slicing.py\u001b[0m in \u001b[0;36mdynamic_slice_in_dim\u001b[0;34m(operand, start_index, slice_size, axis)\u001b[0m\n\u001b[1;32m    696\u001b[0m   \u001b[0mstart_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0mslice_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_canonicalize_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdynamic_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/lax/slicing.py\u001b[0m in \u001b[0;36mdynamic_slice\u001b[0;34m(operand, start_indices, slice_sizes)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mdynamic_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mstatic_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   return dynamic_slice_p.bind(operand, *start_indices, *dynamic_sizes,\n\u001b[0m\u001b[1;32m    111\u001b[0m                               slice_sizes=tuple(static_sizes))\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    358\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    359\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    209\u001b[0m                                     donated_invars, False, *arg_specs)\n\u001b[1;32m    210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleanrl_jax/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1914\u001b[0m           results.consume_token())\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       \u001b[0mout_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisassemble_into_single_device_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from chat GPT\n",
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# from gymnax code\n",
    "def save_pkl_object(obj, filename):\n",
    "    \"\"\"Helper to store pickle objects.\"\"\"\n",
    "    import pickle\n",
    "    from pathlib import Path\n",
    "\n",
    "    output_file = Path(filename)\n",
    "    output_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with open(filename, \"wb\") as output:\n",
    "        # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"Stored data at {filename}.\")\n",
    "\n",
    "\n",
    "def load_pkl_object(filename: str):\n",
    "    \"\"\"Helper to reload pickle objects.\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as input:\n",
    "        obj = pickle.load(input)\n",
    "    print(f\"Loaded data from {filename}.\")\n",
    "    return obj\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        if len(inputs.shape) == 1:\n",
    "            # inputs /= jnp.array([10, 1, 1, 1, 1])\n",
    "            # inputs /= jnp.array([10, 1, 1, 1, 1])\n",
    "            x_0 = nn.Dense(4)(inputs[: -1])\n",
    "            x_1 = nn.Dense(4)(inputs[: -1])\n",
    "            x_1 *= inputs[-1:] # if zero == 0, if one == 1\n",
    "            x_0 *= (inputs[-1:] - 1) ** 2 # if zero == 1, if one == 0\n",
    "            pred = x_0 + x_1\n",
    "        else:\n",
    "            # inputs /= jnp.array([10, 1, 1, 1, 1])\n",
    "            # inputs /= jnp.array([10, 1, 1, 1, 1])\n",
    "            x_0 = nn.Dense(4)(inputs[:, : -1])\n",
    "            x_1 = nn.Dense(4)(inputs[:, : -1])\n",
    "            x_1 *= inputs[:, -1:] # if zero == 0, if one == 1\n",
    "            x_0 *= (inputs[:, -1:] - 1) ** 2 # if zero == 1, if one == 0\n",
    "            pred = x_0 + x_1\n",
    "        # inputs /= jnp.array([10, 1, 1, 1, 1])\n",
    "        # x = nn.relu(nn.Dense(256)(inputs))\n",
    "        # x = nn.relu(nn.Dense(256)(x))\n",
    "        # x = nn.relu(nn.Dense(256)(x))\n",
    "        # pred = nn.Dense(4)(x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "# Define the optimizer (Adam with cosine learning rate schedule and weight decay)\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "# Generate the input and output arrays\n",
    "env = gym.make('CartPole-v1')\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        new_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_states.append(new_obs)\n",
    "        states.append(obs)\n",
    "        obs = new_obs\n",
    "inputs_arr = jnp.concatenate([jnp.array(states, dtype=jnp.float32), jnp.array(actions, dtype=jnp.float32).reshape(-1, 1)], axis=1)\n",
    "outputs_arr = jnp.array(next_states, dtype=jnp.float32)\n",
    "\n",
    "network = TransitionModel()\n",
    "params = network.init(jax.random.PRNGKey(0), inputs_arr[0])\n",
    "# Create a train state object\n",
    "train_state = train_state.TrainState.create(\n",
    "    apply_fn=network.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "@jax.jit\n",
    "def train_step(train_state, x, y):\n",
    "# Define the loss function (mean squared error)\n",
    "    def mse_loss(y_pred, y_true):\n",
    "        diff = y_pred - y_true\n",
    "        return jnp.mean(jnp.sum(diff**2, axis=-1))\n",
    "\n",
    "    def loss_fn_wrapper(params):\n",
    "        y_pred = train_state.apply_fn(params, x)\n",
    "        return mse_loss(y_pred, y)\n",
    "    grad_fn = jax.value_and_grad(loss_fn_wrapper)\n",
    "    loss, grad = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grad)\n",
    "    return train_state, loss\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "num_batches = inputs_arr.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in range(num_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = (batch + 1) * batch_size\n",
    "        batch_inputs = inputs_arr[batch_start:batch_end] # / jnp.array([10, 1, 1, 1, 1])\n",
    "        batch_outputs = outputs_arr[batch_start:batch_end] # / jnp.array([10, 1, 1, 1])\n",
    "        train_state, batch_loss = train_step(train_state, batch_inputs, batch_outputs)\n",
    "        epoch_loss += batch_loss\n",
    "    epoch_loss /= num_batches\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {epoch_loss}\")\n",
    "\n",
    "save_pkl_object({\"params\": train_state.params}, \"forward_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 2.2498042583465576\n",
      "Epoch 2 - Training Loss: 1.894377589225769\n",
      "Epoch 3 - Training Loss: 1.5968819856643677\n",
      "Epoch 4 - Training Loss: 1.3440979719161987\n",
      "Epoch 5 - Training Loss: 1.1280978918075562\n",
      "Epoch 6 - Training Loss: 0.9433769583702087\n",
      "Epoch 7 - Training Loss: 0.7855345010757446\n",
      "Epoch 8 - Training Loss: 0.6508222222328186\n",
      "Epoch 9 - Training Loss: 0.536070704460144\n",
      "Epoch 10 - Training Loss: 0.43867701292037964\n",
      "Stored data at backward_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "# Define the optimizer (Adam with cosine learning rate schedule and weight decay)\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "# Generate the input and output arrays\n",
    "env = gym.make('CartPole-v1')\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        new_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_states.append(new_obs)\n",
    "        states.append(obs)\n",
    "        obs = new_obs\n",
    "inputs_arr = jnp.concatenate([jnp.array(next_states, dtype=jnp.float32), jnp.array(actions, dtype=jnp.float32).reshape(-1, 1)], axis=1)\n",
    "outputs_arr = jnp.array(states, dtype=jnp.float32)\n",
    "\n",
    "network = TransitionModel()\n",
    "params = network.init(jax.random.PRNGKey(0), inputs_arr[0])\n",
    "# Create a train state object\n",
    "train_state = train_state.TrainState.create(\n",
    "    apply_fn=network.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "@jax.jit\n",
    "def train_step(train_state, x, y):\n",
    "# Define the loss function (mean squared error)\n",
    "    def mse_loss(y_pred, y_true):\n",
    "        diff = y_pred - y_true\n",
    "        return jnp.mean(jnp.sum(diff**2, axis=-1))\n",
    "\n",
    "    def loss_fn_wrapper(params):\n",
    "        y_pred = train_state.apply_fn(params, x)\n",
    "        return mse_loss(y_pred, y)\n",
    "    grad_fn = jax.value_and_grad(loss_fn_wrapper)\n",
    "    loss, grad = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grad)\n",
    "    return train_state, loss\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "num_batches = inputs_arr.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in range(num_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = (batch + 1) * batch_size\n",
    "        batch_inputs = inputs_arr[batch_start:batch_end] # / jnp.array([10, 1, 1, 1, 1])\n",
    "        batch_outputs = outputs_arr[batch_start:batch_end] # / jnp.array([10, 1, 1, 1])\n",
    "        train_state, batch_loss = train_step(train_state, batch_inputs, batch_outputs)\n",
    "        epoch_loss += batch_loss\n",
    "    epoch_loss /= num_batches\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {epoch_loss}\")\n",
    "\n",
    "save_pkl_object({\"params\": train_state.params}, \"backward_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanrl_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
